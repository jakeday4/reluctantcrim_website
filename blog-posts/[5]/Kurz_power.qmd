---
title: "Thoughts on Kurz's blog re:power"
author: "Jon Brauer"
date: "2023-04-15"
format:
  html:
    code-fold: true
    code-summary: "Show code"
execute: 
  warning: false
draft: true
---

## Inspiration

I have relied heavily on Solomon Kurz's writings in his translational bookdown projects, so I was excited to continue the journey following his new blog series on causal inferences from RCT designs. Here are my thoughts about the first post, which you can find [here](https://solomonkurz.netlify.app/blog/2023-04-12-boost-your-power-with-baseline-covariates/).

## Reproduction

Let's reproduce his work.

```{r}

# packages
library(tidyverse)
library(broom)

# adjust the global plotting theme
theme_set(theme_gray(base_size = 12) +
            theme(panel.grid = element_blank()))

```

### Data 

Read in data.

```{r}

horan1971 <- tibble(
  sl = c(letters[1:22], letters[1:20], letters[1:19], letters[1:19]),
  sn = 1:80,
  treatment = factor(rep(1:4, times = c(22, 20, 19, 19))),
  pre = c(149.5, 131.25, 146.5, 133.25, 131, 141, 145.75, 146.75, 172.5, 156.5, 153, 136.25, 148.25, 152.25, 167.5, 169.5, 151.5, 165, 144.25, 167, 195, 179.5,
          127, 134, 163.5, 155, 157.25, 121, 161.25, 147.25, 134.5, 121, 133.5, 128.5, 151, 141.25, 164.25, 138.25, 176, 178, 183, 164,
          149, 134.25, 168, 116.25, 122.75, 122.5, 130, 139, 121.75, 126, 159, 134.75, 140.5, 174.25, 140.25, 133, 171.25, 198.25, 141.25,
          137, 157, 142.25, 123, 163.75, 168.25, 146.25, 174.75, 174.5, 179.75, 162.5, 145, 127, 146.75, 137.5, 179.75, 168.25, 187.5, 144.5),
  post = c(149, 130, 147.75, 139, 134, 145.25, 142.25, 147, 158.25, 155.25, 151.5, 134.5, 145.75, 153.5, 163.75, 170, 153, 178, 144.75, 164.25, 194, 183.25,
           121.75, 132.25, 166, 146.5, 154.5, 114, 148.25, 148.25, 133.5, 126.5, 137, 126.5, 148.5, 145.5, 151.5, 128.5, 176.5, 170.5, 181.5, 160.5,
           145.5, 122.75, 164, 118.5, 122, 125.5, 129.5, 137, 119.5, 123.5, 150.5, 125.75, 135, 164.25, 144.5, 135.5, 169.5, 194.5, 142.5,
           129, 146.5, 142.25, 114.5, 148.25, 161.25, 142.5, 174.5, 163, 160.5, 151.25, 144, 135.5, 136.5, 145.5, 185, 174.75, 179, 141.5)) %>% 
  mutate(treatment = factor(treatment, labels = c("delayed", "placebo", "scheduled", "experimental")))

# what is this?
head(horan1971)

```

Subset to two conditions. 

```{r}

horan1971 <- horan1971 %>% 
  filter(treatment %in% c("delayed", "experimental"))

horan1971 %>% 
  count(treatment)

```

Visualize Y_post (post-treatment weight) across treatment (experimental) & control (delayed) groups.

```{r}

horan1971 %>%  
  ggplot(aes(x = post)) +
  geom_histogram(binwidth = 5) +
  xlab("post-treatment weight (lbs)") +
  facet_wrap(~ treatment, labeller = label_both)

```

Mean difference in Y_post across groups. 

```{r}

horan1971 %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(post),
            sd = sd(post),
            n = n(),
            percent_missing = mean(is.na(post)) * 100)

```

Data wrangling. 

```{r}

horan1971 <- horan1971 %>% 
  # make a mean-centered version of pre
  mutate(prec = pre - mean(pre))


horan1971 <- horan1971 %>% 
  mutate(delayed      = ifelse(treatment == "delayed", 1, 0),
         experimental = ifelse(treatment == "experimental", 1, 0))

horan1971 %>% 
  distinct(treatment, delayed, experimental)

```

### ANOVA (OLS-based) model. 

Fit & summarize ANOVA (unconditional mean difference)

```{r}

# fit the ANOVA-type model with OLS
ols1 <- lm(
  data = horan1971,
  post ~ 1 + experimental
)

# summarize
summary(ols1)

```

Isolate key ANOVA estimate & add 95%CI.

```{r}

tidy(ols1, conf.int = TRUE) %>% 
  slice(2) %>% 
  mutate_if(is.double, round, digits = 2)

```

### Introduce baseline covariate, or Y_pre

Baseline covariate, Y_pre (pre-treatment weight), is highly correlated with Y_post outcome

```{r}

horan1971 %>% 
  group_by(treatment) %>% 
  summarise(r = cor(pre, post))

```

```{r}
# for the annotation
r_text <- horan1971 %>% 
  group_by(treatment) %>% 
  summarise(r = cor(pre, post)) %>% 
  mutate(pre  = 130,
         post = 190,
         text = str_c("italic(r)==", round(r, digits = 3)))

# plot!
horan1971 %>%  
  ggplot(aes(x = pre, y = post)) +
  geom_abline(color = "white") +
  geom_point() +
  geom_text(data = r_text,
            aes(label = text),
            color = "red4", parse = TRUE) +
  coord_equal(xlim = c(110, 200),
              ylim = c(110, 200)) +
  facet_wrap(~ treatment, labeller = label_both)
```

Summarize mean of Y_pre 

```{r}
horan1971 %>% 
  summarise(pre_mean = mean(pre),
            pre_sd = sd(pre),
            pre_min = min(pre))

```

### ANCOVA (OLS-based) models. 

Fit & summarize ANCOVA (conditional mean difference) models by stratifying on Y_pre (centered and non-centered versions).

```{r}

# fit with the centered `prec` covariate
ols2 <- lm(
  data = horan1971,
  post ~ 1 + experimental + prec
)

# fit with the non-centered `pre` covariate
ols3 <- lm(
  data = horan1971,
  post ~ 1 + experimental + pre
)

# summarize the centered model
summary(ols2)

# summarize the non-centered model
summary(ols3)

```

Isolate key ANCOVA estimate & add 95%CI. 

```{r}

tidy(ols2, conf.int = TRUE) %>% 
  slice(2) %>% 
  mutate_if(is.double, round, digits = 2)

```

Visualize ANOVA & ANCOVA estimates + intervals. 

```{r}

# wrangle
bind_rows(tidy(ols1, conf.int = TRUE), tidy(ols2, conf.int = TRUE)) %>% 
  filter(term == "experimental") %>% 
  mutate(model = factor(c("ANOVA", "ANCOVA"), levels = c("ANOVA", "ANCOVA"))) %>% 

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist])), expand = expansion(add = 5)) +
  ylab(NULL)

```

## Statistical Power for Free?  

At this point, Kurz states: 

> "Both the ANOVA and ANCOVA models are known to produce unbiased estimates of the population parameters, but the ANCOVA model tends to produce estimates that are more precise (e.g., O’Connell et al., 2017). Thus if you have a high-quality baseline covariate laying around, it’s a good idea to throw it into the model."

He further notes: 

> "...this pattern will arise again and again when you use OLS models. Adding baseline covariates will generally shrink your standard errors. It’s like getting statistical power for free. Perhaps a more helpful line of inquiry is: How can I use this information to better design my studies?"

Statistical power for free! Is this really an exception to the [TNSTAAFL](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch) ("There's no such thing as a free lunch) adage? Before Milton Friedman ressurects [in opposition](https://www.youtube.com/watch?v=YmqoCHR14n8), let's take a closer look at what might be happening here. 

Might this simply be due to fundamental differences between the ANOVA and ANCOVA model estimates? The ANOVA model estimates the unconditional mean difference in post-treatment weight across groups, whereas the ANCOVA model estimates the mean difference in post-treatment weight across groups *at the average pre-treatment weight*. The ANOVA model's standard error (SE) and 95%CIs communicate uncertainty in that estimate due to observed variation in the data, irrespective of the sources of that variation. The ANCOVA model's SE and 95%CIs are estimated from a model that makes more restrictive assumptions about the sources of variation in the data - for instance, it imposes an assumption that the estimated treatment effect is constant across strata of the baseline covariate (i.e., pre-treatment weight). 

Now, imagine there are substantial baseline differences in the distributions of pre-treatment weight across groups, or imagine that the treatment effect actually varies (i.e., there is effect heterogeneity) across levels of pre-treatment weight (Y_pre). Why might stratification on baseline Y_pre values result in smaller CIs and (the illusion of) increased power of the statistical test in the ANCOVA model? Well, for one, if the experimental treatment indeed had different effects for people with different baseline covariate (Y_pre) values, then the current OLS-based ANCOVA model above ignores this existing treatment heterogeneity. In contrast, the ANOVA model still estimates the mean difference between groups in post-treatment weights (Y_post). If there were no baseline distributional differences and the treatment has nearly the same effect on everyone in the treatment group, then I would expect the ANOVA model to estimate that effect relatively precisely (i.e., with narrower CIs). However, if the treatment effect varies, then the ANOVA estimate will necessarily capture that existing variation in its standard error and resulting confidence limits (i.e., with wide CIs). Put simply, the ANOVA model does not have the luxury of adjusting out or restrictively assuming away variation due to sources like effect heterogeneity - it's SE and CIs must capture the existing variation when estimating the average treatment effect across the entire sample. 

Let's see if any of this might be happening in the current example. Below, I will rerun the OLS-based ANCOVA model, but this time I also include an interaction term. My hunch is that the treatment estimate might vary across levels of baseline pre-treatment weight and, if so, then it might do so in a way that as one "slides" the estimates around Y_pre values, the resulting CIs will be captured by the overall ANOVA bounds. Maybe I'm wrong, which might lead me to learn something about power along the way.

### Alternative ANCOVA model

To simplify the process, I will create new Y_pre variables centered at the minimum and maximum pre-treatment weight values, then re-estimate the `ols2` model at the mean(Y_pre), min(Y_pre), and max(Y_pre) values, and finally plot the estimated mean differences alongside the ANOVA estimate. 

```{r}

horan1971 <- horan1971 %>% 
  # make min-centered & max-centered version of pre
  mutate(premin = pre - min(pre), 
         premax = pre - max(pre))


# fit with the Y_pre variables at different locations 
ols4min <- lm(
  data = horan1971,
  post ~ 1 + experimental + premin + experimental*premin
)

ols4c <- lm(
  data = horan1971,
  post ~ 1 + experimental + prec + experimental*prec
)

ols4max <- lm(
  data = horan1971,
  post ~ 1 + experimental + premax + experimental*premax
)


# summarize the centered model
summary(ols4min)

summary(ols4c)

summary(ols4max)

```

Now let's plot these estimates and intervals along with the ANOVA estimate. 

```{r}

#unconditional (ANOVA) & conditional (ANCOVA) mean difference estimates: 

# wrangle
bind_rows(tidy(ols1, conf.int = TRUE), tidy(ols4min, conf.int = TRUE), 
          tidy(ols4c, conf.int = TRUE), tidy(ols4max, conf.int = TRUE)) %>%
  filter(term == "experimental") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "ANCOVA_Ypremax"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "ANCOVA_Ypremax"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist])), expand = expansion(add = 5)) +
  ylab(NULL)

```

To recap, what have I done here? First, I added an interaction term to the ANCOVA model to allow for the possibility of treatment heterogeneity across strata of the baseline covariate. Second, I reestimated the model using pre-treatment variables centered at different locations - that is, at the pre-treatment minimum, mean, and maximum weights - to facilitate generation of the (potentially variable) estimate of the treatment effect at these different baseline covariate values. If there were no treatment effect differences across pre-treatment weight strata, then all three new ANCOVA models would have generated comparable mean difference estimates. However, the model implies there **are** treatment effect differences across different pre-treatment weights, which is not too surprising. After all, those in heavier pre-treatment weight groups (e.g., "Y_premax") at baseline have more to lose in an effective experimental treatment condition than do those in lighter pre-treatment weight groups (e.g., "Y_premin"). 

Likewise, once I take these potential differences into account, the 95% CIs around our ANCOVA model estimates at these three strata together are much wider and more closely overlap the overall ANOVA estimate's bounds. Note, too, that the overlap is not perfect. This is unsurprising given our model specifications. After all, our ANCOVA stratifies on baseline weight and assumes that a particular functional (multiplicative linear) form adequately summarizes the interactive relationship between pre-treatment weight and the experimental treatment effect, while the ANOVA model summarizes post-treatment differences in weight that are due to all possible sources, including potential experimental effects and existing baseline pre-treatment differences. Likewise, in estimating the treatment effect across the strata of the pre-treatment variable (i.e., across different pre-treatment weights), estimation of standard errors and resulting CIs are affected by available data within each strata which, unsurprisingly, are not constant across Y_pre strata.

But is it that simple? Does this imply that an ANOVA and an ANCOVA estimate will have more similar confidence limits if there is no effect heterogeneity? Let's explore this question with a simulation - maybe this will help us build a better intuition for what is happening in the ANOVA and ANCOVA models.

## Simulated Example without Effect Heterogeneity

To generate the simulated data, I will use the observed statistical estimates from the example above as the population parameters in the simulation. For instance, I will first use `rnorm()` to simulate two groups of pre-treatment weights from a normally distributed population of pre-treatment weights summarized by the same group means and standard deviations observed in the "pre" weights above. Then, I use the [`faux`](https://debruine.github.io/faux/) package to simulate post-treatment weights from a population with the same overall mean and standard deviation obsvered for the "post" weights, and correlated with our simulated pre-treatment weights at a population average of *r*=0.95, plus a population-expected treatment effect of -4.57 for the experimental treatment group. 

I will start with very large samples - two groups of n=10,000 each - to ensure our samples reflect the population parameters. Then, I will repeat the simulation with two groups of n=1,000, and finally repeat a third time with two groups of n=22 participants to better match the real-world example above.

### Simulation example #1 (n=10,000/group)

#### Simulate data 

```{r}

# library("faux")
#faux package removed from CRAN
#archive: https://cran.r-project.org/src/contrib/Archive/faux/

# install.packages("groundhog")

library("groundhog")
groundhog.library(faux, "2023-03-01", tolerate.R.version='4.2.2')


#Illustrate sample mean differences for two independent population groups 
nval <- 20000 #treat
ngrp <- 10000
set.seed(1138) 

meanpret <- mean(horan1971$pre[horan1971$treatment=="experimental"])
sdpret <- sd(horan1971$pre[horan1971$treatment=="experimental"])
meanprec <- mean(horan1971$pre[horan1971$treatment=="delayed"])
sdprec <- sd(horan1971$pre[horan1971$treatment=="delayed"])

#sim two pre-treatment columns (groups) of rnorm values with "nval" size 

pretrt <- rnorm(n=nval, mean=meanpret, sd=sdpret) %>% as_tibble_col(column_name = "pre")
precon <- rnorm(n=nval, mean=meanprec, sd=sdprec) %>% as_tibble_col(column_name = "pre")
pre <- rbind(pretrt, precon)

simdata <- tibble(
  sn = 1:nval, 
  treatment = factor(rep(1:2, times = c(ngrp, ngrp)))) %>% 
  mutate(treatment = factor(treatment, labels = c("experimental", "control")),
         experimental = ifelse(treatment == "experimental", 1, 0)
         )  

simdata <- cbind(simdata, pre)

simdata <- simdata %>% 
  mutate(
    post = round(-4.57*experimental + rnorm_pre(pre, mu = mean(horan1971$post), sd = sd(horan1971$post), r = 0.95), digits=2), 
    prec = pre - mean(pre), 
    premin = pre - min(pre), 
    premax = pre - max(pre)
  )

head(simdata)

```

Summarize & plot Y_pre distributions

```{r}

simdata %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(pre),
            sd = sd(pre),
            n = n())

simdata %>%  
  ggplot(aes(x = pre)) +
  geom_histogram(binwidth = 5) +
  xlab("pre-treatment weight (lbs)") +
  facet_wrap(~ treatment, labeller = label_both)

```

Summarize & plot Y_post distributions

```{r}

simdata %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(post),
            sd = sd(post),
            n = n())

simdata %>%  
  ggplot(aes(x = post)) +
  geom_histogram(binwidth = 5) +
  xlab("post-treatment weight (lbs)") +
  facet_wrap(~ treatment, labeller = label_both)

```

Pre-post correlation

```{r}
simdata %>% 
  group_by(treatment) %>% 
  summarise(r = cor(pre, post))
```

#### Fit models

Let's check the ANOVA estimate first.  

```{r}
# ANOVA (OLS)
simols1 <- lm(
  data = simdata,
  post ~ 1 + experimental
)

summary(simols1)

tidy(simols1, conf.int = TRUE) %>% 
  slice(2) %>% 
  mutate_if(is.double, round, digits = 2)

```

Our ANOVA estimate (-4.49) is close to the population specified treatment effect (-4.57). Given how large our sample is, the confidence intervals [-4.82, -4.15] might seem somewhat imprecise. 

Now, let's fit the ANCOVA models and allow for the possibility of treatment effect heterogeneity across baseline pre-treatment weights. We did not specify effect heterogeneity in the population, so we do not expect our large samples to show evidence of much, if any, interaction.   

```{r}

# ANCOVA (OLS) with interaction to capture any effect heterogeneity  
simols4min <- lm(
  data = simdata,
  post ~ 1 + experimental + premin + experimental*premin
)

simols4c <- lm(
  data = simdata,
  post ~ 1 + experimental + prec + experimental*prec
)

simols4max <- lm(
  data = simdata,
  post ~ 1 + experimental + premax + experimental*premax
)


# summarize the centered model
summary(simols4min)

summary(simols4c)

summary(simols4max)


#Plot mean difference estimates: 

# wrangle
bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE)) %>%
  filter(term == "experimental") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "ANCOVA_Ypremax"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "ANCOVA_Ypremax"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist])), expand = expansion(add = 5)) +
  ylab(NULL) + 
  ggtitle("Simulation #1, n=10,000 per group") 


```

The first thing to note is that all four point estimates - the unconditional ANOVA estimate and the three conditional ANCOVA estimates at the maximum, mean, and minimum values of the baseline covariate - are very similar. This is unsurprising since I did not specify any population treatment effect heterogeneity when generating the simulated data, which means we forced the data to conform to an expectation that the treatment has the same effect on weight irrespective of one's pre-treatment baseline weight. 

The next thing to note is that the ANCOVA estimate of the conditional mean difference in post-treatment weights between experimental and baseline groups is far more precise - that is, it has much smaller confidence interval - at the mean value of the Y_pre variable (Yprec) compared to the ANOVA estimate. This is the same pattern referenced in Kurz's post sometimes interpreted as free precision or statistical power. Moreover, a glance at the model summary output shows its point estimate (-4.52) to be a little closer to the population-specified treatment effect (-4.57) than was the ANOVA estimate (-4.49). 

Now, note that the other two ANCOVA estimates have *wider* confidence intervals compared to the unconditional ANOVA estimate. Essentially, these models communicate greater uncertainty in estimating treatment effects at the tails of the baseline covariate's distribution, where data are more sparse. This should help remind us that ANCOVA model *estimates and uncertainty are conditional* on the location of covariates; the ANOVA estimate, in contrast, is *unconditional*, meaning it does not depend on any specific value of a covariate in the model. 

Conceptually, it might be helpful to think of the ANOVA estimate as summarizing our best guess of the group difference in the outcome (post-treatment weights) that might be due to any and all sources combined - and as summarizing our uncertainty in that best guess given the total amount of outcome variation (e.g., spread of post-treatment weights) observed across both groups. In our example, this means the ANOVA estimate of post-treatment differences is affected by both of the things that we specified as causally relevant in our simulation: (1) it is affected by the average group differences that are due to experimental treatment effects (-4.57*experimental) and any random sampling variation around that  average treatment differences, and (2) by any average baseline differences in pre-treatment weight that existed due to research design and/or random sampling variation. 

Hence, if what we really care about is an estimate of the causal effect of an experimental treatment on an outcome, then the ANOVA's unconditional estimate is not a very good fit for that aim. The ANCOVA model, though, helps us adjust out or cleanse our unconditional estimate of average differences and sources of variation/uncertainty that are caused by things that are not our experimental treatment. For more information about the relationship between ANOVA and the average treatment effect, [check out this paper](https://www.tandfonline.com/doi/abs/10.1080/00273171.2022.2068122).)

Now, let's rerun the simulation with smaller groups of n=1,000 each.

### Simulation example #2 (n=1,000/group)

```{r}

#Illustrate sample mean differences for two independent population groups 
nval <- 2000 #treat
ngrp <- 1000
set.seed(1138) 

#sim two pre-treatment columns (groups) of rnorm values with "nval" size 

pretrt <- rnorm(n=nval, mean=meanpret, sd=sdpret) %>% as_tibble_col(column_name = "pre")
precon <- rnorm(n=nval, mean=meanprec, sd=sdprec) %>% as_tibble_col(column_name = "pre")
pre <- rbind(pretrt, precon)

simdata <- tibble(
  sn = 1:nval, 
  treatment = factor(rep(1:2, times = c(ngrp, ngrp)))) %>% 
  mutate(treatment = factor(treatment, labels = c("experimental", "control")),
         experimental = ifelse(treatment == "experimental", 1, 0)
         )  

simdata <- cbind(simdata, pre)

simdata <- simdata %>% 
  mutate(
    post = round(-4.57*experimental + rnorm_pre(pre, mu = mean(horan1971$post), sd = sd(horan1971$post), r = 0.95), digits=2), 
    prec = pre - mean(pre), 
    premin = pre - min(pre), 
    premax = pre - max(pre)
  )

# ANOVA (OLS)
simols1 <- lm(
  data = simdata,
  post ~ 1 + experimental
)

# ANCOVA (OLS) with interaction to capture any effect heterogeneity  
simols4min <- lm(
  data = simdata,
  post ~ 1 + experimental + premin + experimental*premin
)

simols4c <- lm(
  data = simdata,
  post ~ 1 + experimental + prec + experimental*prec
)

simols4max <- lm(
  data = simdata,
  post ~ 1 + experimental + premax + experimental*premax
)


#Plot mean difference estimates: 

# wrangle
bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE)) %>%
  filter(term == "experimental") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "ANCOVA_Ypremax"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "ANCOVA_Ypremax"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist])), expand = expansion(add = 5)) +
  ylab(NULL) + 
  ggtitle("Simulation #2, n=1,000 per group") 


```

In the second example, we still have pretty large samples, so what the heck happened here? Let's check summaries of the pre-treatment weight distributions, as well as summary outputs from the ANOVA model and the ANCOVA model with the centered pre-treatment weight baseline covariate, to see if they can shed any light on this.

Summarize & plot Y_pre distributions

```{r}

simdata %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(pre),
            sd = sd(pre),
            n = n())

simdata %>%  
  ggplot(aes(x = pre)) +
  geom_histogram(binwidth = 5) +
  xlab("pre-treatment weight (lbs)") +
  facet_wrap(~ treatment, labeller = label_both)

summary(simols1)
summary(simols4c)

```

Compared to our first simulation example (n=10,000/group), the ANOVA and ANCOVA (at centered baseline) estimates are more divergent in this second example with smaller groups (n=1,000 each). The ANOVA estimate (-5.51) is substantially off the population mark (-4.57), whereas the ANCOVA estimate is much closer (-4.62). Why? 

Well, recall that the ANOVA model provides and *unconditional* estimate of the post-treatment differences between groups - ignoring any potential reasons for those differences, such as pre-treatment sampling variability or confounding. Yet, there is a small pre-treatment difference in weights due to sampling variability: the experimental group started off as approximately one pound (0.96) lighter on average than the control group before treatment. If we add this pre-existing difference to our population-expected treatment effect, we would expect an unconditional estimate of the group difference in post-treatment weights to be close to (-4.57 - 0.96) -5.53 pounds. Likewise, the ANOVA estimate is close, at -5.51 pounds. In short, baseline differences in pre-treatment weight shifts our ANOVA estimate about 1 unit (lbs) to the left in our plot.

Again, the unconditional ANOVA estimate summarizes post-treatment differences in weight from *all* sources. In contrast, the ANCOVA model stratifies on the centered baseline covariate to provide an estimate of the post-treatment differences in weights conditional on a specific strata of the baseline covariate (i.e., at the average pre-treatment weight). After holding constant pre-treatment weight at the baseline covariate's mean, the ANCOVA model better estimates (-4.62) the population-expected post-treatment difference in group weight attributable to the treatment (-4.57). (Though the ANCOVA model estimate would also be confounded by any other unmeasured causes of difference, had we included any such confounding causes in our simulation).   

Finally, note that the ANCOVA estimates vary somewhat across strata of the baseline covariate (i.e., across minimum, mean, and maximum pre-treatment values). Recall, I did not specify any such differences in the simulation; rather, these appeared due to sampling variation - and as samples get smaller, such "apparent interactions" are more likely to occur as artifacts of sampling variation. 

Now, let's run this one more time with smaller group sizes of n=22 each to provide a better match to the real-world example used in Kurz's post. 

### Simulation example #3 (n=22/group)

```{r}

#Illustrate sample mean differences for two independent population groups 
nval <- 44 #treat
ngrp <- 22
set.seed(1138) 

#sim two pre-treatment columns (groups) of rnorm values with "nval" size 

pretrt <- rnorm(n=nval, mean=meanpret, sd=sdpret) %>% as_tibble_col(column_name = "pre")
precon <- rnorm(n=nval, mean=meanprec, sd=sdprec) %>% as_tibble_col(column_name = "pre")
pre <- rbind(pretrt, precon)

simdata <- tibble(
  sn = 1:nval, 
  treatment = factor(rep(1:2, times = c(ngrp, ngrp)))) %>% 
  mutate(treatment = factor(treatment, labels = c("experimental", "control")),
         experimental = ifelse(treatment == "experimental", 1, 0)
         )  

simdata <- cbind(simdata, pre)

simdata <- simdata %>% 
  mutate(
    post = round(-4.57*experimental + rnorm_pre(pre, mu = mean(horan1971$post), sd = sd(horan1971$post), r = 0.95), digits=2), 
    prec = pre - mean(pre), 
    premin = pre - min(pre), 
    premax = pre - max(pre)
  )

# ANOVA (OLS)
simols1 <- lm(
  data = simdata,
  post ~ 1 + experimental
)

# ANCOVA (OLS) with interaction to capture any effect heterogeneity  
simols4min <- lm(
  data = simdata,
  post ~ 1 + experimental + premin + experimental*premin
)

simols4c <- lm(
  data = simdata,
  post ~ 1 + experimental + prec + experimental*prec
)

simols4max <- lm(
  data = simdata,
  post ~ 1 + experimental + premax + experimental*premax
)



# Summarize & plot Y_pre distributions

simdata %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(pre),
            sd = sd(pre),
            n = n())

simdata %>%  
  ggplot(aes(x = pre)) +
  geom_histogram(binwidth = 5) +
  xlab("pre-treatment weight (lbs)") +
  facet_wrap(~ treatment, labeller = label_both)

summary(simols1)
summary(simols4c)


#Plot mean difference estimates: 

# wrangle
bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE)) %>%
  filter(term == "experimental" | term == "prec") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "Baseline_diff_prec", "ANCOVA_Ypremax"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", "Baseline_diff_prec", "ANCOVA_Ypremax"))) %>%
  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist])), expand = expansion(add = 5)) +
  ylab(NULL) + 
  ggtitle("Simulation #3, n=22 per group") 

```

First, note that at pre-treatment baseline the simulated experimental group (159.41) is about 5.75 pounds heavier on average than the control group (mean=153.66). In the above plot, I added this estimated baseline difference to the plot ("Baseline_diff_prec"), as it might help us visualize what is happening in the ANOVA estimate. Specifically, the baseline pre-treatment weight difference should shift the ANOVA estimate of post-treatment differences to the right, since the ANOVA model provides an unconditional estimate that essentially is an aggregated summary of all sources of post-treatment differences in weight - including treatment effects and pre-existing weight differences. Sure enough, the ANOVA model estimate and the coefficient plots bear out this expectation. 

Second, we see the same pattern of differences in uncertainty across ANCOVA estimates where the baseline covariate is centered at different values (e.g., at the minimum, mean, or maximum pre-treatment  weight). Conditional ANCOVA estimates are just that - they are conditional on specific values of the covariate - and thus they tend to be more certain at the center of a distribution and are less certain at the tails of a covariate distribution. Meanwhile, the ANOVA model's unconditional estimate summarizes post-treatment differences and variation/uncertainty from all sources, including experimental effects and baseline differences in pre-treatment weight.  

### On sampling variation and average treatment effects

Now, let's wrap up with a couple more illustrations using simulation #3. First, we will use the `margins` package to add an estimate of the experiment's average marginal effect (AME). Kurz's first post does not really get into this, though it sound like he plans to discuss marginal effects in later posts in the series. As such, we will not get into details here. For now, you might think of the AME conceptually as follows: It is the effect estimate that we would get if we followed our approach of fitting an ANCOVA model with new baseline covariates that were recentered at all observed values of pre-treatment weight, extracted the estimates and uncertainty intervals from each of those models, and then averaged all these into a single estimate and corresponding uncertainty interval. 

Under certain conditions, such as a linear model with no effect heterogeneity, the ANCOVA estimate from a model with a baseline covariate centered at its mean will converge with the AME estimate and interval. However, in other situations, such as when effect heterogeneity is present (i.e., interactions exist) and/or when fitting a generalized linear model with a nonlinear link function, these estimates will not always converge. For now, let's just add them to our plot to provide a bit more information.   

```{r}
library("margins")
x <- lm(post ~ 1 + experimental + prec + experimental*prec, data = simdata)

margins_summary(x)

amecoef <- tidy(x, conf.int = TRUE) %>% filter(term == "experimental")

# wrangle
plot22_1138 <- bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE), 
          amecoef) %>%
  filter(term == "experimental") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                           "ANCOVA_Ypremax", "AME_treatment"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                                   "ANCOVA_Ypremax", "AME_treatment"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist]))) +
  coord_cartesian(xlim=c(-20,10)) +
  ylab(NULL) + 
  ggtitle("Sim #3, seed=1138") 

plot22_1138

```

As implied, the AME estimate converges with the ANCOVA estimate using the centered baseline covariate in this example. But what if we had a different random sample? Interestingly, this particular random sample shows no evidence of effect heterogeneity, even though random sampling variation led to such an artifact in simulation #2 with the larger samples (n=1,000 per group). As you might know, with small samples, we should expect a great deal of sampling variation and, as such, noisy estimates that jump around a lot. To some degree, the wider intervals in the ANOVA and ANCOVA at minimum and maximum baseline covariate values communicates this greater uncertainty. But what about the ANCOVA estimate at centered baseline weight or the AME - do these sufficiently convey uncertainty inherent in estimating from small samples? 

To probe this intuition, let's change the random seed to draw a couple new samples and then generate the same plot. I started with "1138," which is one of my [favorite nerdy random seeds](https://en.wikipedia.org/wiki/1138_(number)). Let's just add one, add two, and subtract one to the random seed and see what happens.  

```{r}
#Illustrate sample mean differences for two independent population groups 
nval <- 44 #treat
ngrp <- 22

# 1139 

set.seed(1139) 

#sim two pre-treatment columns (groups) of rnorm values with "nval" size 

pretrt <- rnorm(n=nval, mean=meanpret, sd=sdpret) %>% as_tibble_col(column_name = "pre")
precon <- rnorm(n=nval, mean=meanprec, sd=sdprec) %>% as_tibble_col(column_name = "pre")
pre <- rbind(pretrt, precon)

simdata <- tibble(
  sn = 1:nval, 
  treatment = factor(rep(1:2, times = c(ngrp, ngrp)))) %>% 
  mutate(treatment = factor(treatment, labels = c("experimental", "control")),
         experimental = ifelse(treatment == "experimental", 1, 0)
         )  

simdata <- cbind(simdata, pre)

simdata <- simdata %>% 
  mutate(
    post = round(-4.57*experimental + rnorm_pre(pre, mu = mean(horan1971$post), sd = sd(horan1971$post), r = 0.95), digits=2), 
    prec = pre - mean(pre), 
    premin = pre - min(pre), 
    premax = pre - max(pre)
  )

# ANOVA (OLS)
simols1 <- lm(
  data = simdata,
  post ~ 1 + experimental
)

# ANCOVA (OLS) with interaction to capture any effect heterogeneity  
simols4min <- lm(
  data = simdata,
  post ~ 1 + experimental + premin + experimental*premin
)

simols4c <- lm(
  data = simdata,
  post ~ 1 + experimental + prec + experimental*prec
)

simols4max <- lm(
  data = simdata,
  post ~ 1 + experimental + premax + experimental*premax
)

x <- lm(post ~ 1 + experimental + prec + experimental*prec, data = simdata)

amecoef2 <- tidy(x, conf.int = TRUE) %>% filter(term == "experimental")

# wrangle
plot22_1139 <- bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE), 
          amecoef2) %>%
  filter(term == "experimental") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                           "ANCOVA_Ypremax", "AME_treatment"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                                   "ANCOVA_Ypremax", "AME_treatment"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist]))) +
  coord_cartesian(xlim=c(-20,10)) +
  ylab(NULL) + 
  ggtitle("Sim #3, seed=1139") 

# 1140

set.seed(1140) 

#sim two pre-treatment columns (groups) of rnorm values with "nval" size 

pretrt <- rnorm(n=nval, mean=meanpret, sd=sdpret) %>% as_tibble_col(column_name = "pre")
precon <- rnorm(n=nval, mean=meanprec, sd=sdprec) %>% as_tibble_col(column_name = "pre")
pre <- rbind(pretrt, precon)

simdata <- tibble(
  sn = 1:nval, 
  treatment = factor(rep(1:2, times = c(ngrp, ngrp)))) %>% 
  mutate(treatment = factor(treatment, labels = c("experimental", "control")),
         experimental = ifelse(treatment == "experimental", 1, 0)
         )  

simdata <- cbind(simdata, pre)

simdata <- simdata %>% 
  mutate(
    post = round(-4.57*experimental + rnorm_pre(pre, mu = mean(horan1971$post), sd = sd(horan1971$post), r = 0.95), digits=2), 
    prec = pre - mean(pre), 
    premin = pre - min(pre), 
    premax = pre - max(pre)
  )

# ANOVA (OLS)
simols1 <- lm(
  data = simdata,
  post ~ 1 + experimental
)

# ANCOVA (OLS) with interaction to capture any effect heterogeneity  
simols4min <- lm(
  data = simdata,
  post ~ 1 + experimental + premin + experimental*premin
)

simols4c <- lm(
  data = simdata,
  post ~ 1 + experimental + prec + experimental*prec
)

simols4max <- lm(
  data = simdata,
  post ~ 1 + experimental + premax + experimental*premax
)

x <- lm(post ~ 1 + experimental + prec + experimental*prec, data = simdata)

amecoef3 <- tidy(x, conf.int = TRUE) %>% filter(term == "experimental")

# wrangle
plot22_1140 <- bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE), 
          amecoef3) %>%
  filter(term == "experimental" ) %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                           "ANCOVA_Ypremax", "AME_treatment"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                                   "ANCOVA_Ypremax", "AME_treatment"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist]))) +
  coord_cartesian(xlim=c(-20,10)) +
  ylab(NULL) + 
  ggtitle("Sim #3, seed=1140") 


# 1137

set.seed(1137) 

#sim two pre-treatment columns (groups) of rnorm values with "nval" size 

pretrt <- rnorm(n=nval, mean=meanpret, sd=sdpret) %>% as_tibble_col(column_name = "pre")
precon <- rnorm(n=nval, mean=meanprec, sd=sdprec) %>% as_tibble_col(column_name = "pre")
pre <- rbind(pretrt, precon)

simdata <- tibble(
  sn = 1:nval, 
  treatment = factor(rep(1:2, times = c(ngrp, ngrp)))) %>% 
  mutate(treatment = factor(treatment, labels = c("experimental", "control")),
         experimental = ifelse(treatment == "experimental", 1, 0)
         )  

simdata <- cbind(simdata, pre)

simdata <- simdata %>% 
  mutate(
    post = round(-4.57*experimental + rnorm_pre(pre, mu = mean(horan1971$post), sd = sd(horan1971$post), r = 0.95), digits=2), 
    prec = pre - mean(pre), 
    premin = pre - min(pre), 
    premax = pre - max(pre)
  )

# ANOVA (OLS)
simols1 <- lm(
  data = simdata,
  post ~ 1 + experimental
)

# ANCOVA (OLS) with interaction to capture any effect heterogeneity  
simols4min <- lm(
  data = simdata,
  post ~ 1 + experimental + premin + experimental*premin
)

simols4c <- lm(
  data = simdata,
  post ~ 1 + experimental + prec + experimental*prec
)

simols4max <- lm(
  data = simdata,
  post ~ 1 + experimental + premax + experimental*premax
)

x <- lm(post ~ 1 + experimental + prec + experimental*prec, data = simdata)

amecoef4 <- tidy(x, conf.int = TRUE) %>% filter(term == "experimental")

# wrangle
plot22_1137 <- bind_rows(tidy(simols1, conf.int = TRUE), tidy(simols4min, conf.int = TRUE), 
          tidy(simols4c, conf.int = TRUE), tidy(simols4max, conf.int = TRUE), 
          amecoef4) %>%
  filter(term == "experimental") %>%
  mutate(model = factor(c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                          "ANCOVA_Ypremax", "AME_treatment"), 
                        levels = c("ANOVA", "ANCOVA_Ypremin", "ANCOVA_Yprec", 
                                   "ANCOVA_Ypremax", "AME_treatment"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist]))) +
  coord_cartesian(xlim=c(-20,10)) +
  ylab(NULL) +
  ggtitle("Sim #3, seed=1137") 


library(patchwork)

(plot22_1137 + plot22_1138) / (plot22_1139 + plot22_1140)



```

Oh my - look at the differences across these plots! This is what happens when we use really small samples - our samples and sample-based estimates are noisy and unreliable. If we were interested in effect heterogeneity, such as differences in the experimental treatment effect for people with different baseline pre-treatment weights, then we would likely draw completely different inferences from each of these samples! 

Also, note how the ANOVA estimate jumps around a lot. By now, this should be unsurprising. After all, recall the unconditional ANOVA estimate is summarizing post-treatment group differences from all causes, including from highly variable sample-dependent group differences in baseline weight and  treatment effects. Moreover, this is compounded by the fact that ANOVA's mean-based estimate is sensitive to another problem with small samples: their distributions often fail to adequately reflect the underlying population distributions from which they were drawn. Graefe and colleagues, [in their paper referenced earlier](file:///C:/Users/Jon/Downloads/GraefeHahnMayer2022.pdf), warn about relying on ANOVA to estimate an average treatment effect (ATE): 

> "For the ATE being a meaningful effect, however, a representative sample or knowledge of the distribution of the covariate is essential. If the distribution of the covariate in the sample does not reflect the distribution in the population of interest, all approaches may result in incorrect estimations of ATEs and the computed average treatment effects themselves do not represent average causal effects but arbitrary computed differences between means. In this case, conditional treatment effects can and should be interpreted."

Likewise, note that the ANCOVA with a centered baseline covariate and the AME estimates are relatively more stable across these plots. Let's extract the AME estimates and plot them together so we can more easily compare them. 

```{r}
bind_rows(amecoef, amecoef2, amecoef3, amecoef4) %>%
  mutate(model = factor(c("AME_1138", "AME_1139", "AME_1140", "AME_1137"), 
                        levels = c("AME_1138", "AME_1139", "AME_1140", "AME_1137"))) %>%

  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = model)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange() +
  scale_x_continuous(expression(beta[1]~(mu[experimental]-mu[waitlist]))) +
  ylab(NULL) +
  ggtitle("Sim #3, AMEs by random seed")
```

These findings corroborate the quote above and confirm Kurz's advice that "...if you have a high-quality baseline covariate laying around, it’s a good idea to throw it into the model."  

The reason, though, really is not because you can expect ANCOVA to improve the statistical power of your test. To be clear, you might expect a derived estimated of the statistical power of an ANCOVA-based test of a null hypothesis to be greater than the power estimate derived for an ANOVA-based test of a null hypothesis that was motivated by the same substantive question. However, at this point, it should be clear that the ANOVA and ANCOVA models might be generating fundamentally different estimates or "tests," depending on your data and the causal and design processes that generated them. Rather, if you care about estimating the causal effect of a (randomized) experimental treatment, then what really matters is that you fit a model that adequately specifies the underlying causal structure that generated the data and that you stratify on or adjust for alternative (non-treatment) causes of post-treatment differences if any exist. Inclusion of baseline differences - or other approaches to analyzing within-unit change - can help in this regard by serving as a proxy to adjust for other unobserved causal processes that might generate observed differences in post-treatment outcomes above and beyond the experimental treatment effect of interest.  

In short, count me as a skeptic of power-based arguments for selecting any particular model or determining covariate inclusion and, instead, as a believer that there is still no such thing as a free (power) lunch. 
